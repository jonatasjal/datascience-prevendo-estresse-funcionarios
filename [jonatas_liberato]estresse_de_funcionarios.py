# -*- coding: utf-8 -*-
"""[JONATAS-LIBERATO]estresse-de-funcionarios.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u7gBn8iWDlCy3LeeCa5YUMPJM59bqq7O

# **IDENTIFICANDO ESTRESSE DE FUNCIONÁRIOS**

# Sobre o projeto

Sabemos que estresse, juntamente com ansiedade e depressão são ameaças para a saúde mental de qualquer pessoa.

-

Por muitas vezes, a pessoa estressada com ocasiões do seu dia a dia acabam por usar as redes sociais para desabafar ou até mesmo buscar orientações (não profissionais).

-

É aí que muitas empresas viram uma oportunidade para descobrir, através das redes sociais, se as pessoas estão com problemas e oferecerem ajuda.

-

É pensando nisso que desenvolveremos uma máquina preditiva para indentificar como anda o estresse dos usuários.

# Problema de Negócio

Um dos maiores desafios é identificar, em meio a tantas palavras nas postagens, se o usuário está estressado psicologicamente ou não.

-

Para criar uma máquina e modelar o seu aprendizado, usaremos o segundo dataset:

https://raw.githubusercontent.com/amankharwal/Website-data/master/stress.csv

-

O dataset possui dados postados em **subreddits** que estão relacionados a saúde mental. Ele ainda contém** 116 colunas**, mas usaremos somente a coluna de **texto** e de **rótulo**. 

-

No dataset ainda estão presentes vários problema de saúde mental que foram compartilhados pelos usuários e sua classificação é **0 (sem estresse)** e **1 (estressado)**.

# 1. Análise Exploratória
"""

# Bibliotecas
import pandas as pd
import numpy as np

# Dados
dataset = pd.read_csv("https://raw.githubusercontent.com/amankharwal/Website-data/master/stress.csv")
dataset

dataset.info()

dataset.describe()

dataset.shape

# Dados null
print(dataset.isnull().sum())

"""# 2. Pré-Processamento

**Utilizando NLTK (Processamento de Linguagem Natural)**
"""

import nltk 
import re # expressões regulares

"""Desejamos atribuir um peso para as palavras para conseguirmos definir se em uma frase podemos identificar stress do funcionários.

Faremos uma filtragem das palavras como eliminar pontuações, protocolos web, etc. 
"""

nltk.download('stopwords')
stemmer = nltk.SnowballStemmer("english")

from nltk.corpus import stopwords
import string

stopword=set(stopwords.words('english'))

# Função criada para limpar o texto
def clean(text):
    text = str(text).lower() # letras minúsculas
    text = re.sub('\[.*?\]', '', text) 
    text = re.sub('https?://\S+|www\.\S+', '', text) # eliminando https
    text = re.sub('<.*?>+', '', text) # eliminando pontuação
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = [word for word in text.split(' ') if word not in stopword]
    text=" ".join(text)
    text = [stemmer.stem(word) for word in text.split(' ')]
    text=" ".join(text)
    return text
dataset["text"] = dataset["text"].apply(clean)

"""Aqui criamos uma wordcloud para visualizar as palavras mais aparecem no texto."""

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

text = " ".join(i for i in dataset.text)
stopwords = set(STOPWORDS)
wordcloud = WordCloud(stopwords=stopwords, 
                      background_color="white").generate(text)
                      
plt.figure( figsize=(15,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

""" Faremos algumas transformações para facilitar a análise."""

# Substituindo 0 por 'No Stress' e 1 por 'Stress'
dataset['label'] = dataset['label'].map({0: 'No Stress', 1: 'Stress'})
dataset = dataset[['text', 'label']]
print(dataset.head)

# Cada palavras se torna uma variável
dataset.text[0]

# Criando os dados de treino e teste
from sklearn.feature_extraction.text import CountVectorizer # atribui peso às variáveis
from sklearn.model_selection import train_test_split

x = np.array(dataset["text"])
y = np.array(dataset["label"])

cv = CountVectorizer() # aplicado em cada uma das palavras
X = cv.fit_transform(x) # fazendo a transformação

# Treinamento dos dados
xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)

"""# 3 - Máquina Preditiva"""

# Nessa máquina preditiva usamos o NaiveBayes de Bernoulli
from sklearn.naive_bayes import BernoulliNB
model = BernoulliNB()
model.fit(xtrain, ytrain)

"""# 4 - Avaliando a Máquina"""

# Teste 1
usuario = input("Digite o seu texto em inglês: ")
data = cv.transform([usuario]).toarray()
output = model.predict(data)
print(output)

# Teste 2
usuario = input("Digite o seu texto em inglês: ")
data = cv.transform([usuario]).toarray()
output = model.predict(data)
print(output)

#Autor: Jonatas A. Liberato
#Ref: Eduardo Rocha